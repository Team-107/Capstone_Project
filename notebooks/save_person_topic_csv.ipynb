{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-opera",
   "metadata": {},
   "source": [
    "Notebook works in tandem with save_topic_metadata.ipynb.\n",
    "\n",
    "Purpose of this notebook is to import the primary SIPP 2018 dataset limited to the columns within a topic area, take a subset of that data to collapse to the person level, and finally save that smaller dataframe to the data directory.\n",
    "\n",
    "The second part of this notebook reads all csv's created above and merges them and the final output is saved to csv. In short, the raw original dataframe becomes a dataframe with fewer rows collapsed to person level and fewer features.\n",
    "\n",
    "Purpose of this process is create a manageable file to use for EDA and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-trade",
   "metadata": {},
   "source": [
    "### Create Person Level CSV's for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-statement",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "# Load Schema for import ----------\n",
    "rd_schema = pd.read_json('../data/raw/sipp_2018/pu2018_schema.json')\n",
    "rd_schema['dtype'] = (['Int64' if x == 'integer'\n",
    "                       else 'object' if x == 'string'\n",
    "                       else 'Float64' if x == 'float'\n",
    "                       else 'ERROR'\n",
    "                       for x in rd_schema['dtype']]\n",
    "                     )\n",
    "\n",
    "# Define core features ----------\n",
    "core_features = ['SSUID', 'PNUM', 'MONTHCODE', \n",
    "                 'RIN_UNIV', 'TAGE', 'EOWN_ST',\n",
    "                ]\n",
    "\n",
    "# Get list of metadata files with prefix from directory\n",
    "path = '../data/interim/'\n",
    "prefix = 'feature_import_meta_'\n",
    "file_list = [x for x in os.listdir(path) if prefix in x]  # List includes on files with prefix in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file\n",
    "for file in file_list:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    ## read first column and put list of column names in list\n",
    "    topic_features = (pd.read_csv(path+file, usecols=['variable'])\n",
    "                      .variable\n",
    "                      .to_list()\n",
    "                     )\n",
    "    \n",
    "    ## Combine core cols with list from file\n",
    "    all_features = core_features + topic_features\n",
    "    \n",
    "    ## Remove duplicate features\n",
    "    use_cols = []\n",
    "    [use_cols.append(x) for x in all_features if x not in use_cols]                      \n",
    "   \n",
    "    ## Read csv w/ dask\n",
    "    dask_topic_df = dd.read_csv(\"../data/raw/sipp_2018/pu2018.csv\",\n",
    "                                usecols=use_cols,\n",
    "                                dtype=dict([(i,v) for i,v in zip(rd_schema.name, rd_schema.dtype)]),\n",
    "                                sep='|',\n",
    "                                header=0,\n",
    "                                names=rd_schema['name'],\n",
    "                               )\n",
    "    \n",
    "    ## Subset masks\n",
    "    mask_month = dask_topic_df.MONTHCODE == 12\n",
    "    mask_age = dask_topic_df.TAGE >= 18\n",
    "    mask_univ = dask_topic_df.RIN_UNIV > 0\n",
    "    masks = mask_month & mask_age & mask_univ\n",
    "    \n",
    "    ## Collapse rows, remove columns, and save to csv\n",
    "    filename = file.replace('feature_import_meta', 'sipp2018_person')\n",
    "    \n",
    "    ## Compute to pd.df\n",
    "    dask_topic_person_df = (dask_topic_df\n",
    "                            [masks]    \n",
    "                            .compute()   \n",
    "                           )\n",
    "    ## Edit and save csv\n",
    "    dask_topic_person_df = (dask_topic_person_df\n",
    "                            .drop_duplicates(['SSUID', 'PNUM'])\n",
    "                            .drop(['MONTHCODE', 'RIN_UNIV'], axis='columns')                           \n",
    "                           )\n",
    "    \n",
    "    dask_topic_person_df.to_csv(path + filename, index=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'{filename} iteration time: {end - start} seconds.')\n",
    "    print(f'{filename} saved with {dask_topic_person_df.shape[0]} rows and {dask_topic_person_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-sterling",
   "metadata": {},
   "source": [
    "### Merge Collapsed Person Level CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new empty df\n",
    "sipp_2018 = pd.DataFrame()\n",
    "\n",
    "# read csv's\n",
    "path = '../data/interim/sipp2018_person'\n",
    "file_list = os.listdir(path)\n",
    "df_list = [pd.read_csv(f'{path}/{file}') for file in file_list if 'sipp_person_' in file]\n",
    "\n",
    "# merge dataframes\n",
    "for i, df in enumerate(df_list):\n",
    "    try:\n",
    "        sipp_2018 = pd.merge(sipp_2018, df.drop('EOWN_ST', axis=1), on=['SSUID', 'PNUM'])\n",
    "    except:\n",
    "        sipp_2018 = df\n",
    "        print(i)\n",
    "        \n",
    "# save merged df to csv\n",
    "sipp_2018.to_csv(f'{path}/sipp2018_person.csv', index=False)\n",
    "\n",
    "sipp_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights columns added after the fact. This was run independently on July 13\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "# Load Schema for import ----------\n",
    "rd_schema = pd.read_json('../data/raw/sipp_2018/pu2018_schema.json')\n",
    "rd_schema['dtype'] = (['Int64' if x == 'integer'\n",
    "                       else 'object' if x == 'string'\n",
    "                       else 'Float64' if x == 'float'\n",
    "                       else 'ERROR'\n",
    "                       for x in rd_schema['dtype']]\n",
    "                     )\n",
    "\n",
    "weights = pd.read_csv(\"../data/raw/sipp_2018/pu2018.csv\",\n",
    "                      names=rd_schema['name'],\n",
    "                      dtype=dict([(i,v) for i,v in zip(rd_schema.name, rd_schema.dtype)]),\n",
    "                      sep='|',\n",
    "                      header=0,\n",
    "                      usecols=['SSUID', 'PNUM', 'MONTHCODE', \n",
    "                               'RIN_UNIV', 'TAGE', 'EOWN_ST', 'WPFINWGT',\n",
    "                              ],\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_person = (weights\n",
    "                    .sort_values(by=['SSUID', 'PNUM', 'MONTHCODE'], \n",
    "                                 ascending=[True, True, False])\n",
    "                    .drop_duplicates(['SSUID', 'PNUM'])\n",
    "                    .query('RIN_UNIV > 0 and TAGE >= 18')\n",
    "                    .drop(['MONTHCODE', 'RIN_UNIV'], axis='columns')\n",
    "                   )\n",
    "weights_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_person = weights_person.drop(['EOWN_ST', 'TAGE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "corporate-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_person['SSUID'] = weights_person['SSUID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "sipp_person = pd.read_csv('../data/interim/sipp2018_person/sipp2018_person.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equal-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "sipp_person = pd.merge(sipp_person, weights_person, on=['SSUID', 'PNUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "identical-slope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WPFINWGT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5989.598574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3904.848315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4082.881038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3994.372476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3994.372476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49699</th>\n",
       "      <td>3690.255482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49700</th>\n",
       "      <td>3340.674724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49701</th>\n",
       "      <td>4522.314071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49702</th>\n",
       "      <td>3347.512844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49703</th>\n",
       "      <td>3965.376597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49704 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          WPFINWGT\n",
       "0      5989.598574\n",
       "1      3904.848315\n",
       "2      4082.881038\n",
       "3      3994.372476\n",
       "4      3994.372476\n",
       "...            ...\n",
       "49699  3690.255482\n",
       "49700  3340.674724\n",
       "49701  4522.314071\n",
       "49702  3347.512844\n",
       "49703  3965.376597\n",
       "\n",
       "[49704 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sipp_person.filter(like='WPFIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "metric-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sipp_person.to_csv('../data/interim/sipp2018_person/sipp2018_person.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-crown",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team 107 Capstone Project",
   "language": "python",
   "name": "team_107_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
